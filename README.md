Transportation End-to-End ETL Pipeline using Databricks & PySpark
ğŸ“Œ Project Overview

This project implements a scalable end-to-end Transportation ETL pipeline using Databricks, PySpark, Delta Live Tables (DLT), Delta Lake, and Amazon S3.
The pipeline ingests raw transportation data, processes it through Bronzeâ€“Silverâ€“Gold (Medallion Architecture) layers, and produces analytics-ready datasets for reporting and business insights.

The solution demonstrates modern data engineering best practices, including incremental ingestion, data quality validation, CDC-based upserts, and optimized cloud storage.

ğŸ¯ Business Problem

Transportation companies generate large volumes of data from multiple sources such as trips, cities, passengers, and dates.
However:

Data is often raw, inconsistent, and unstructured

Traditional systems do not scale well

Poor data quality leads to unreliable analytics

This project solves the problem by building a scalable, automated ETL pipeline that converts raw data into clean, trusted, analytics-ready datasets.

ğŸ—ï¸ Architecture Overview

The project follows the Medallion Architecture:

Bronze Layer â€“ Raw data ingestion from Amazon S3 (fault-tolerant, schema-flexible)

Silver Layer â€“ Cleaned, validated, standardized data with CDC upserts

Gold Layer â€“ Business-friendly, analytics-ready views for reporting

All layers are implemented using Delta Live Tables and stored as Delta tables on Amazon S3.

ğŸ§° Technologies Used

Databricks

Apache Spark (PySpark)

Delta Live Tables (DLT)

Delta Lake

Amazon S3

AWS IAM

SQL

ğŸ”„ Data Pipeline Flow

Raw transportation data is ingested from Amazon S3 into the Bronze layer

Corrupt records are captured without failing the pipeline

Data is cleaned, standardized, and validated in the Silver layer

CDC (Change Data Capture) is used to upsert latest records

Gold views are created by joining Silver tables for analytics

ğŸ“Š Key Features

Incremental data ingestion

Schema evolution support

Data quality checks using DLT expectations

CDC-based upserts (SCD Type 1)

Auto-optimized Delta tables

Analytics-ready Gold views

Scalable cloud-based architecture

ğŸ“ Project Structure
â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ city_bronze.py
â”‚   â”œâ”€â”€ trips_bronze.py
â”‚
â”œâ”€â”€ silver/
â”‚   â”œâ”€â”€ city_silver.py
â”‚   â”œâ”€â”€ trips_silver_cdc.py
â”‚   â”œâ”€â”€ calendar_silver.py
â”‚
â”œâ”€â”€ gold/
â”‚   â”œâ”€â”€ fact_trips_view.sql
â”‚
â”œâ”€â”€ README.md

ğŸ§ª Example Analytics Use Cases

Average passenger and driver ratings

Trips by city and date

Weekend vs weekday trip analysis

Holiday-based demand analysis

Monthly and quarterly revenue trends

ğŸš€ How to Run the Project

Create a Databricks workspace on AWS

Configure access to Amazon S3 using IAM roles

Create a Delta Live Tables pipeline

Add Bronze, Silver, and Gold notebooks

Provide pipeline parameters (e.g., start_date, end_date)

Run the pipeline

ğŸ”® Future Enhancements

Real-time streaming ingestion

BI dashboards using Power BI / QuickSight

Advanced analytics and ML integration

SCD Type 2 for historical tracking

Enhanced data governance and lineage

ğŸ“Œ Conclusion

This project demonstrates a production-style data engineering pipeline using Databricks and Delta Live Tables.
It showcases how raw transportation data can be transformed into reliable, analytics-ready datasets using modern lakehouse architecture and cloud-native tools.

ğŸ‘¤ Author

Tejas Jadhav
Software Engineer | Aspiring Data Engineer
Skills: Databricks, PySpark, Delta Lake, AWS, SQL
